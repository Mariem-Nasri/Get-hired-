{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "NKG32XtxCOBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing :"
      ],
      "metadata": {
        "id": "JclkiH19Cdhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DataSet :"
      ],
      "metadata": {
        "id": "IgvBB6WuCf2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8YJwOtDvmjn",
        "outputId": "97c4b79f-aa55-44f0-b38f-bed49882f42d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Set file paths\n",
        "resumes_df = pd.read_csv('/content/drive/MyDrive/Resume DataSet/Resume.csv')\n"
      ],
      "metadata": {
        "id": "mjQoWKl7vrSJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installations :"
      ],
      "metadata": {
        "id": "GtERjlwQDAGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXynIf0txkEK",
        "outputId": "ec07b5f4-5538-4cbc-f5ab-d19b3b2f21ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports :"
      ],
      "metadata": {
        "id": "E68iJIAKDJDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import _treebank_word_tokenizer\n",
        "from nltk.data import load\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "4Uo-GcVBDNAH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####NLTK SetUp :"
      ],
      "metadata": {
        "id": "Q4rpMWJHDRkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab', download_dir='/root/nltk_data')\n",
        "\n",
        "# Forced load 'punkt'\n",
        "nltk.data.path.append('/root/nltk_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g5EC_SyDgTx",
        "outputId": "d5105921-a48f-40f7-adf2-c2c0231dd446"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CLEANING + TOKENIZATION + LEMMATIZATION"
      ],
      "metadata": {
        "id": "R-viyuy9EO33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Basic cleaning function\n",
        "# ---------------------------\n",
        "def clean_text_spacy(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and len(token) > 2]\n",
        "    return list(dict.fromkeys(tokens))  # remove duplicates\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Initial cleaning for resumes\n",
        "# ---------------------------\n",
        "resumes_df['temp_cleaned'] = resumes_df['Resume_str'].astype(str).apply(clean_text_spacy)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Automatically detect common resume words\n",
        "# ---------------------------\n",
        "all_resume_tokens = [token for sublist in resumes_df['temp_cleaned'] for token in sublist]\n",
        "resume_counts = Counter(all_resume_tokens)\n",
        "num_resumes = len(resumes_df)\n",
        "resume_threshold = 0.5\n",
        "common_resume_words = [word for word, count in resume_counts.items() if count / num_resumes > resume_threshold]\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Final cleaning for resumes\n",
        "# ---------------------------\n",
        "def clean_resume(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc\n",
        "              if not token.is_stop and len(token) > 2 and token.lemma_ not in common_resume_words]\n",
        "    return list(dict.fromkeys(tokens))\n",
        "\n",
        "resumes_df['Resume_clean'] = resumes_df['Resume_str'].astype(str).apply(clean_resume)\n",
        "resumes_df.drop(columns=['temp_cleaned'], inplace=True)\n",
        "\n",
        "\n",
        "print(\"✅ First cleaned resume:\")\n",
        "print(resumes_df[['Resume_str', 'Resume_clean']].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Uhua7w4ET6X",
        "outputId": "435307ef-6450-479a-91a2-564df90c29a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ First cleaned resume:\n",
            "Resume_str               HR ADMINISTRATOR/MARKETING ASSOCIATE\\...\n",
            "Resume_clean    [administrator, marketing, associate, dedicate...\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iheXO5vGHm4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# 5. SECTION EXTRACTION (regex-based)\n",
        "# -----------------------------------------\n",
        "# Keep your dictionary of regex patterns for sections\n",
        "section_patterns = {\n",
        "    \"education\": r'education\\b|degree|university|college|diploma',\n",
        "    \"experience\": r'experience\\b|work history|employment',\n",
        "    \"skills\": r'skills?\\b|competencies|expertise',\n",
        "    \"projects\": r'projects?\\b|portfolio|initiative',\n",
        "    \"certifications\": r'certifications?|licenses?|awards?',\n",
        "    \"languages\": r'languages?\\b|english|french|arabic|german',\n",
        "}\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"\n",
        "    Extract key sections from resumes.\n",
        "    Uses regex patterns.\n",
        "    If no match → return 'NOT FOUND'\n",
        "    \"\"\"\n",
        "    sections = {}\n",
        "    for name, pattern in section_patterns.items():\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        sections[name] = match.group(0) if match else \"NOT FOUND\"\n",
        "    return sections\n",
        "\n",
        "# Apply extraction\n",
        "resumes_df['Sections'] = resumes_df['Resume_str'].apply(extract_sections)\n"
      ],
      "metadata": {
        "id": "GL50ovcMGNeB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_full_sections(text):\n",
        "    \"\"\"\n",
        "    Extract text for each section in a resume.\n",
        "    Returns a dictionary {section_name: section_text}.\n",
        "    \"\"\"\n",
        "    sections = {}\n",
        "    text = text.lower()  # normalize\n",
        "    # Find positions of all section headers\n",
        "    header_positions = []\n",
        "    for name, pattern in section_patterns.items():\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            header_positions.append((match.start(), name))\n",
        "    # Sort headers by position\n",
        "    header_positions.sort()\n",
        "\n",
        "    # Extract content between headers\n",
        "    for i, (start, name) in enumerate(header_positions):\n",
        "        end = header_positions[i + 1][0] if i + 1 < len(header_positions) else len(text)\n",
        "        section_text = text[start:end].strip()\n",
        "        sections[name] = section_text\n",
        "    # Fill missing sections with NOT FOUND\n",
        "    for name in section_patterns.keys():\n",
        "        if name not in sections:\n",
        "            sections[name] = \"NOT FOUND\"\n",
        "    return sections\n"
      ],
      "metadata": {
        "id": "dkGZHEpVIkYY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections_expanded = resumes_df['Sections'].apply(pd.Series)\n",
        "resumes_df = pd.concat([resumes_df, sections_expanded], axis=1)\n",
        "\n",
        "# Now you can view it like a table\n",
        "print(resumes_df[['Resume_str', 'education', 'experience', 'skills', 'projects', 'certifications', 'languages']].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlYHUiF3IHlI",
        "outputId": "f3449bc8-3e52-4a0d-d46d-091cedff9ca1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          Resume_str   education   education  \\\n",
            "0           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   Education   Education   \n",
            "1           HR SPECIALIST, US HR OPERATIONS      ...   Education   Education   \n",
            "2           HR DIRECTOR       Summary      Over 2...  University  University   \n",
            "3           HR SPECIALIST       Summary    Dedica...   Education   Education   \n",
            "4           HR MANAGER         Skill Highlights  ...   Education   Education   \n",
            "\n",
            "   experience  experience     skills     skills    projects    projects  \\\n",
            "0  experience  experience     skills     skills   NOT FOUND   NOT FOUND   \n",
            "1  Experience  Experience     Skills     Skills  initiative  initiative   \n",
            "2  experience  experience     Skills     Skills     Project     Project   \n",
            "3  experience  experience  expertise  expertise   NOT FOUND   NOT FOUND   \n",
            "4  Employment  Employment      Skill      Skill     Project     Project   \n",
            "\n",
            "  certifications certifications  languages  languages  \n",
            "0  Certification  Certification  NOT FOUND  NOT FOUND  \n",
            "1      NOT FOUND      NOT FOUND  NOT FOUND  NOT FOUND  \n",
            "2         Awards         Awards  NOT FOUND  NOT FOUND  \n",
            "3      NOT FOUND      NOT FOUND  NOT FOUND  NOT FOUND  \n",
            "4      NOT FOUND      NOT FOUND  NOT FOUND  NOT FOUND  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EMBEDDING MODELS :"
      ],
      "metadata": {
        "id": "DKTiA1cqJZF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####CBOW :"
      ],
      "metadata": {
        "id": "CMpLTk9jJ8Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# ---- CBOW ----\n",
        "class CBOWEmbedder:\n",
        "    def __init__(self, vector_size=768, window=5, min_count=2):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def train(self, texts):\n",
        "        # Split cleaned text into tokens for training\n",
        "        sentences = [t.split() for t in texts if len(str(t)) > 0]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size,\n",
        "                              window=self.window, min_count=self.min_count, sg=0)\n",
        "        return self.model\n",
        "\n",
        "    def get_doc_embedding(self, text):\n",
        "        # Average of word vectors\n",
        "        tokens = text.split()\n",
        "        vecs = [self.model.wv[t] for t in tokens if t in self.model.wv]\n",
        "        return np.mean(vecs, axis=0) if vecs else np.zeros(self.vector_size)\n"
      ],
      "metadata": {
        "id": "rgwhzJMjJYDv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Skip-gram :\n"
      ],
      "metadata": {
        "id": "ttbIj7cgJlnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramEmbedder(CBOWEmbedder):\n",
        "    def train(self, texts):\n",
        "        # texts is a list of token lists\n",
        "        sentences = [t for t in texts if len(t) > 0]\n",
        "\n",
        "        # Initialize the Skip-gram Word2Vec model\n",
        "        self.model = Word2Vec(\n",
        "            sentences,\n",
        "            vector_size=768,  # 768-dimensional embeddings\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            sg=1  # sg=1 for Skip-gram\n",
        "        )\n",
        "        return self.model\n"
      ],
      "metadata": {
        "id": "75ZyGvJhJo-P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BERT :"
      ],
      "metadata": {
        "id": "TX6-QKl-Jp0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- BERT ----\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class BERTEmbedder:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_embedding(self, tokens):\n",
        "        # Join token list into a string\n",
        "        text = \" \".join(tokens)\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        # Mean pooling over tokens\n",
        "        mask = inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "        sum_emb = torch.sum(outputs.last_hidden_state * mask, 1)\n",
        "        sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "        return (sum_emb / sum_mask).squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "eOh3p8U7Jp9v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models results :"
      ],
      "metadata": {
        "id": "QSOBLuQCJyU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = resumes_df['Resume_clean'].tolist()\n",
        "\n",
        "# CBOW\n",
        "cbow = CBOWEmbedder()\n",
        "cbow.train(texts)\n",
        "emb1 = cbow.get_doc_embedding(texts[0])\n",
        "\n",
        "# Skip-gram\n",
        "texts = resumes_df['Resume_clean'].tolist()  # list of token lists\n",
        "\n",
        "sg = SkipGramEmbedder()\n",
        "sg.train(texts)\n",
        "emb_sg = sg.get_doc_embedding(texts[0])\n",
        "\n",
        "\n",
        "# BERT\n",
        "bert = BERTEmbedder()\n",
        "emb_bert = bert.get_embedding(texts[0])\n",
        "print(\"BERT shape:\", emb_bert.shape)\n",
        "\n",
        "print(\"CBOW shape:\", emb1.shape)\n",
        "print(\"Skip-gram shape:\", emb_sg.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "0urJ6O7bJ2F_",
        "outputId": "7be30b6d-80c9-4c74-de64-a361e858ab49"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1926900314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# CBOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOWEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0memb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1805999536.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Split cleaned text into tokens for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         self.model = Word2Vec(sentences, vector_size=self.vector_size,\n\u001b[1;32m     17\u001b[0m                               window=self.window, min_count=self.min_count, sg=0)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Similarity"
      ],
      "metadata": {
        "id": "5-mLG5g1z4hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# CV-related word pairs with human similarity scores\n",
        "word_pairs = [\n",
        "    (\"python\", \"java\"),\n",
        "    (\"developer\", \"programmer\"),\n",
        "    (\"accountant\", \"engineer\"),\n",
        "    (\"excel\", \"spreadsheet\"),\n",
        "    (\"analyst\", \"manager\")\n",
        "]\n",
        "human_scores = [0.8, 0.9, 0.3, 0.85, 0.5]\n",
        "\n",
        "# Evaluation for Word2Vec models (CBOW / Skip-gram)\n",
        "def evaluate_similarity(model, pairs, human_scores):\n",
        "    model_scores = []\n",
        "    for (w1, w2) in pairs:\n",
        "        if w1 in model.wv and w2 in model.wv:\n",
        "            sim = model.wv.similarity(w1, w2)\n",
        "        else:\n",
        "            sim = 0\n",
        "        model_scores.append(sim)\n",
        "    corr, _ = spearmanr(human_scores, model_scores)\n",
        "    return corr, model_scores\n",
        "\n",
        "print(\"CBOW:\", evaluate_similarity(cbow.model, word_pairs, human_scores))\n",
        "print(\"Skip-gram:\", evaluate_similarity(sg.model, word_pairs, human_scores))\n",
        "\n",
        "# BERT similarity\n",
        "def bert_similarity(bert, w1, w2):\n",
        "    v1 = bert.get_embedding(w1).reshape(1, -1)\n",
        "    v2 = bert.get_embedding(w2).reshape(1, -1)\n",
        "    return cosine_similarity(v1, v2)[0][0]\n",
        "\n",
        "bert_scores = [bert_similarity(bert, w1, w2) for (w1, w2) in word_pairs]\n",
        "print(\"BERT similarity scores:\", bert_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eDhVBTebv3pE",
        "outputId": "deb08152-710c-4e63-a999-17da4a3d769e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cbow' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3760337255.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CBOW:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skip-gram:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cbow' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Word Analogies"
      ],
      "metadata": {
        "id": "1-FhHGI9z9GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example resume-domain analogies\n",
        "\n",
        "# Developer specializations\n",
        "print(\"Analogy: developer - java + python ≈ ?\")\n",
        "print(\"CBOW:\", cbow.model.wv.most_similar(positive=['developer','python'], negative=['java'], topn=3))\n",
        "print(\"Skip-gram:\", sg.model.wv.most_similar(positive=['developer','python'], negative=['java'], topn=3))\n",
        "\n",
        "# Data career path\n",
        "print(\"\\nAnalogy: data - analyst + scientist ≈ ?\")\n",
        "print(\"CBOW:\", cbow.model.wv.most_similar(positive=['data','scientist'], negative=['analyst'], topn=3))\n",
        "print(\"Skip-gram:\", sg.model.wv.most_similar(positive=['data','scientist'], negative=['analyst'], topn=3))\n",
        "\n",
        "# Machine learning engineer\n",
        "print(\"\\nAnalogy: machine - learning + engineer ≈ ?\")\n",
        "print(\"CBOW:\", cbow.model.wv.most_similar(positive=['machine','engineer'], negative=['learning'], topn=3))\n",
        "print(\"Skip-gram:\", sg.model.wv.most_similar(positive=['machine','engineer'], negative=['learning'], topn=3))\n"
      ],
      "metadata": {
        "id": "62fCKIUOzp7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is not naturally trained for analogies; you'd need vector arithmetic but it’s less effective"
      ],
      "metadata": {
        "id": "pK0uUq2b0Swv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Clustering"
      ],
      "metadata": {
        "id": "nQvH1GXu0XnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose CV-related words\n",
        "words = [\"developer\", \"designer\", \"manager\", \"analyst\", \"engineer\",\n",
        "         \"scientist\", \"consultant\", \"intern\"]\n",
        "\n",
        "# Get vectors from CBOW\n",
        "X = [cbow.model.wv[w] for w in words if w in cbow.model.wv]\n",
        "\n",
        "# Dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "coords = pca.fit_transform(X)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(coords[:,0], coords[:,1], c='blue')\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (coords[i,0], coords[i,1]))\n",
        "\n",
        "plt.title(\"CBOW Word Clustering (CV Job Roles)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TOV4Hx8w0TSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "words = [\"developer\", \"designer\", \"manager\", \"analyst\", \"engineer\",\n",
        "         \"scientist\", \"consultant\", \"intern\"]\n",
        "\n",
        "# Get vectors from Skip-gram\n",
        "X = [sg.model.wv[w] for w in words if w in sg.model.wv]\n",
        "\n",
        "coords = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(coords[:,0], coords[:,1], c='red')\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (coords[i,0], coords[i,1]))\n",
        "\n",
        "plt.title(\"Skip-gram Word Clustering (CV Job Roles)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FkX4Re1i0tFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "words = [\"developer\", \"designer\", \"manager\", \"analyst\", \"engineer\",\n",
        "         \"scientist\", \"consultant\", \"intern\"]\n",
        "\n",
        "# Get embeddings from BERT\n",
        "X = [bert.get_embedding(w) for w in words]\n",
        "\n",
        "coords = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(coords[:,0], coords[:,1], c='purple')\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (coords[i,0], coords[i,1]))\n",
        "\n",
        "plt.title(\"BERT Word Clustering (CV Job Roles)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D2cT9wO91C3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extrinsic Evaluation"
      ],
      "metadata": {
        "id": "4_tjCTLc6tOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = [cbow.get_doc_embedding(t) for t in texts]\n",
        "y = resumes_df['Category']  # assuming you have labels like \"Data Science\", \"HR\", etc.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "print(\"CBOW accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "RZ3zwVdu0k9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sg = [sg.get_doc_embedding(t) for t in texts]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sg, y, test_size=0.2, random_state=42)\n",
        "clf_sg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "print(\"Skip-gram accuracy:\", accuracy_score(y_test, clf_sg.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "cVrnY3gj1P0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bert = [bert.get_embedding(t) for t in texts]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bert, y, test_size=0.2, random_state=42)\n",
        "clf_bert = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "print(\"BERT accuracy:\", accuracy_score(y_test, clf_bert.predict(X_test)))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1r8MI05f1SP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vector Comparisons"
      ],
      "metadata": {
        "id": "Fm5KO1ZI7bk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "\n",
        "def compare_vecs(v1, v2):\n",
        "    cos = np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "    euc = norm(v1 - v2)\n",
        "    return cos, euc\n",
        "\n",
        "# Compare embeddings (emb1 = CBOW, emb2 = Skip-gram, emb3 = BERT)\n",
        "pairs = {\n",
        "    \"CBOW vs Skip-gram\": (emb1, emb2),\n",
        "    \"CBOW vs BERT\": (emb1, emb3),\n",
        "    \"Skip-gram vs BERT\": (emb2, emb3)\n",
        "}\n",
        "\n",
        "for name, (v1, v2) in pairs.items():\n",
        "    cos, euc = compare_vecs(v1, v2)\n",
        "    print(f\"{name} -> Cosine similarity: {cos:.4f}, Euclidean distance: {euc:.4f}\")\n"
      ],
      "metadata": {
        "id": "LbZQIYBD7dI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}