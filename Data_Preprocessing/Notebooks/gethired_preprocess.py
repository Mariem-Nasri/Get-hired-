# -*- coding: utf-8 -*-
"""GetHired_Preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/120nBqXGST8W4hio41aaN6o9_rfdBsOoy

#Imports
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from collections import Counter
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

tqdm.pandas()

from google.colab import drive
drive.mount('/content/drive')

"""#Data acquisition

###Resume Data
"""

resume = '/content/drive/MyDrive/GetHired/Resume.csv'
resume = pd.read_csv(resume)

resume.columns

#getting the Resume_html only
resume = resume[['Resume_html']]

#Data shape
resume.shape

print("duplicates in resume dataset : " , resume.duplicated().sum())

#Drop duplicates
resume = resume.drop_duplicates()
print("duplicates in resume dataset : " , resume.duplicated().sum())

#Nan or empty rows in resume
print("Nan or empty rows in resume dataset : " , resume.isnull().sum().sum())

#save resume_html
resume.to_csv('/content/drive/MyDrive/GetHired/resume_html.csv', index=False)

"""Description preview"""

for i, text in enumerate(resume['Resume_html'][:1]):
    print(f"{i}: {text}\n")

"""###Job description data"""

#read the job_dataset_full.csv data from MyDrive
job_desc = '/content/drive/MyDrive/GetHired/job_dataset_full.csv'
job_desc = pd.read_csv(job_desc)

job_desc.columns

job_desc.head()

#Data shape
job_desc.shape

job_desc['Role'].nunique()

#Duplicates in job data
print("duplicates in job dataset : " , job_desc.duplicated().sum())

"""#Data preprocessing

###Functions :

**üßπ Extract Section Names Function Description**  

This function extracts the titles of sections from an HTML resume or document:  
* It parses the given HTML text using BeautifulSoup.
* Looks for all <div> elements with the class "section".
* Inside each of those, it finds a nested <div> with the class "sectiontitle".
* It then extracts and cleans the text of those titles.
* Returns a list of section names (e.g., "Education", "Experience", "Skills").
"""

def extract_section_names(html_text):
    if not isinstance(html_text, str) or not html_text.strip():
        return []

    soup = BeautifulSoup(html_text, "html.parser")
    sections = []

    # Each section has a <div class="section">
    for section_div in soup.find_all("div", class_="section"):
        # Find its section title (inside <div class="sectiontitle">)
        title_div = section_div.find("div", class_="sectiontitle")
        if title_div:
            title = title_div.get_text(strip=True)
            if title:
                sections.append(title)

    return sections

"""**üßπ Segment Resume Sections Function Description**

This function extracts and organizes the main content of a resume into structured sections from its HTML format.

- Parses the HTML using BeautifulSoup.
- Finds all <div> elements with the class "section".
- For each section, retrieves its title (inside <div class="sectiontitle">).
- Uses a predefined dictionary (SECTION_MAP) to standardize section names (e.g., mapping "Work Experience" ‚Üí "experience").
- Collects the text content of each section (excluding the title).
- If multiple sections map to the same standard category, it merges their text.
- Returns a dictionary where keys are standardized section names
"""

def segment_resume_sections(html_text):

    if not isinstance(html_text, str) or not html_text.strip():
        return {}

    soup = BeautifulSoup(html_text, "html.parser")
    segments = {}

    for section_div in soup.find_all("div", class_="section"):
        title_div = section_div.find("div", class_="sectiontitle")
        if not title_div:
            continue

        raw_title = title_div.get_text(strip=True)
        mapped_title = SECTION_MAP.get(raw_title)
        if not mapped_title:
            continue  # skip if not in your lexical list

        # Extract section content (excluding the title div)
        text_parts = []
        for child in section_div.find_all(recursive=False):
            if "sectiontitle" not in child.get("class", []):
                text_parts.append(child.get_text(" ", strip=True))

        full_text = " ".join(text_parts).strip()

        # Merge if multiple sections map to the same standard category
        if mapped_title in segments:
            segments[mapped_title] += " " + full_text
        else:
            segments[mapped_title] = full_text

    return segments

"""**üßπ Text Cleaning Function Description**

This function clean_text(text) preprocesses raw text data (like resumes or job descriptions) to make it cleaner and more consistent for NLP tasks. It removes non-ASCII characters, URLs, emails, HTML tags, and very long numbers while preserving meaningful patterns like C++, C#, and .NET. The function also converts text to lowercase. It temporarily replaces dots in decimal numbers and in ‚Äú.NET‚Äù with placeholders to prevent them from being removed, then restores them later. It keeps letters, digits, spaces, and key symbols (-, +, #) relevant to technical or professional text. Finally, it normalizes extra spaces and returns a cleaned version of the text, ready for tokenization or model input.
"""

def clean_text(text):
    text = text.encode("ascii", "ignore").decode()

    text = re.sub(r'(\d+)\.(\d+)', r'\1<dot>\2', text)
    text = re.sub(r'\.NET', r'<dot>NET', text, flags=re.I)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z0-9\s\-+#]', ' ', text)
    text = re.sub(r'\d{4,}', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace('<dot>', '.')
    text = text.lower()

    return text

"""###Resume preprocessing

####Section segmentation
"""

resume['section_names'] = resume['Resume_html'].progress_apply(extract_section_names)

all_titles = [title for sublist in resume["section_names"] for title in sublist]
section_counts = Counter(all_titles)

# Show top 20 most common variants
for title, count in section_counts.most_common(20):
    print(f"{title}: {count}")

# Lexical mapping
SECTION_MAP = {
    # Summary / Profile
    "Summary": "Summary",
    "Professional Summary": "Summary",
    "Executive Profile": "Summary",

    # Experience
    "Experience": "Experience",
    "Work History": "Experience",
    "Professional Experience": "Experience",

    # Skills
    "Skills": "Skills",
    "Skill Highlights": "Skills",
    "Core Qualifications": "Skills",
    "Highlights": "Skills",

    # Education / Training
    "Education": "Education",
    "Education and Training": "Education",
    "Certifications": "Education",

    # Achievements
    "Accomplishments": "Achievements",

    # Affiliations
    "Affiliations": "Affiliations",
    "Professional Affiliations": "Affiliations",

    # Other / Miscellaneous
    "Additional Information": "Other",
    "Personal Information": "Other",
    "Languages": "Other",
    "Interests": "Other"
}

resume["segmented"] = resume["Resume_html"].progress_apply(segment_resume_sections)

resume_id = 0

print(f"Resume ID {resume_id}:\n")
for section, content in resume.loc[resume_id, "segmented"].items():
    print(f"--- {section.upper()} ---\n{content}\n")

"""####Normalization"""

resume["cleaned_segments"] = resume["segmented"].progress_apply(
    lambda seg: {k: clean_text(v) for k, v in seg.items()} if isinstance(seg, dict) else {}
)

resume_id = 300

print(f"Resume ID {resume_id}:\n")
for section, content in resume.loc[resume_id, "cleaned_segments"].items():
    print(f"--- {section.upper()} ---\n{content}\n")

"""####unifing text for resume"""

def make_structured_resume_text(segments):
    if not isinstance(segments, dict):
        return ""
    parts = [f"[{section}]: {text}" for section, text in segments.items() if text.strip()]
    return " ".join(parts)

resume["unified_text"] = resume["cleaned_segments"].apply(make_structured_resume_text)

resume_id = 300
print(resume.loc[resume_id, "unified_text"])


resume[["unified_text"]].to_csv(""/content/drive/MyDrive/GetHired/job_structured.csv"", index=False)

"""###Job description preprocessing

####Normalization
"""

# Apply clean_text to every text column in job_desc
for col in job_desc.columns:
    if job_desc[col].dtype == "object":
        job_desc[col] = job_desc[col].astype(str).progress_apply(clean_text)

job_desc.head()

"""####unifing text for job description"""

def make_structured_job_desc_text(row):
    parts = []
    for col in job_desc.columns:
        if job_desc[col].dtype == "object":
            val = str(row[col]).strip()
            if val and val.lower() != "nan":
                parts.append(f"[{col}]: {val}")
    return " ".join(parts)

job_desc["unified_text"] = job_desc.apply(make_structured_job_desc_text, axis=1)

job_id = 0
print(job_desc.loc[job_id, "unified_text"])


job_desc[["unified_text"]].to_csv("/content/drive/MyDrive/GetHired/job_structured.csv", index=False)